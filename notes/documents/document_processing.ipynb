{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc09898a",
   "metadata": {},
   "source": [
    "# Document Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59f2bf",
   "metadata": {},
   "source": [
    "## Techniques for Processing a Thousand-Paged Document\n",
    "\n",
    "When working with large documents such as [Computer Networks by Andrew S. Tanenbaum](https://csc-knu.github.io/sys-prog/books/Andrew%20S.%20Tanenbaum%20-%20Computer%20Networks.pdf), efficient processing is essential. Here are the main techniques we will use:\n",
    "\n",
    "### 1. Chunking\n",
    "- **Definition:** Splitting the document into smaller, manageable sections (chunks) such as paragraphs, pages, or chapters.\n",
    "- **Purpose:** Enables parallel processing, easier indexing, and targeted analysis.\n",
    "\n",
    "### 2. Multimodal Processing\n",
    "- **Definition:** Combining text, images, tables, and diagrams for comprehensive understanding.\n",
    "- **Purpose:** Extracts information from both textual and visual elements, improving accuracy and context.\n",
    "\n",
    "### 3. Preprocessing Pipelines\n",
    "- **Definition:** Sequential steps to clean and prepare data, including OCR, tokenization, normalization, and noise removal.\n",
    "- **Purpose:** Ensures consistent and high-quality input for downstream tasks like summarization, search, or classification.\n",
    "\n",
    "---\n",
    "\n",
    "By integrating these techniques, we can efficiently analyze, summarize, and extract insights from extensive technical documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24d1ba",
   "metadata": {},
   "source": [
    "## Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d54bcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.pdf not found. Downloading...\n",
      "Total extracted text length: 7537 characters\n",
      "\n",
      "Using DeepInfra API with model: Qwen/Qwen3-Embedding-8B\n",
      "Processing 1 paragraphs...\n",
      "\n",
      "Embedding batch 1 / 7 (size: 8)\n",
      "Embedding batch 2 / 7 (size: 8)\n",
      "Embedding batch 3 / 7 (size: 8)\n",
      "Embedding batch 4 / 7 (size: 8)\n",
      "Embedding batch 5 / 7 (size: 8)\n",
      "Embedding batch 6 / 7 (size: 8)\n",
      "Embedding batch 7 / 7 (size: 4)\n",
      "\n",
      "✅ Generated 4 semantic chunks.\n",
      "\n",
      "Chunk 1 (Start Index: 0)\n",
      "Content (preview): This is a sample document to showcase page-based formatting. It contains a chapter from a Wikibook called Sensory Systems . None of the content has been changed in this article, but some content has been remo ved.Anat omy of the Somat osensor y System FROM WIKIBOOKS1 Our somatosensor y system c onsi...\n",
      "\n",
      "Chunk 2 (Start Index: 4164)\n",
      "Content (preview): Wide content, lik e the table and Figure 3, intrude into the outside margins.only to intense mechanical stimuli, but also to heat and to no xious chemicals. These rec eptors respond to minute punc tures of the epithelium, with a response magnitude that depends on the degree of tissue def ormation. T...\n",
      "\n",
      "Chunk 3 (Start Index: 5693)\n",
      "Content (preview): The ends of the intrafusal fibers are at tached to e xtrafusal fibers, so when- ever the muscle is stretched, the intrafusal fibers are also stretched. The c entral region of each intrafusal fiber hasAnatom y of the Somatosensory System 3 Force control signal Driving signal Length control signalLoad...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain components\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import spacy  # For sentence splitting\n",
    "\n",
    "# === Load Environment Variables ===\n",
    "load_dotenv()\n",
    "DEEPINFRA_API_KEY = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "if not DEEPINFRA_API_KEY:\n",
    "    raise ValueError(\"Please set DEEPINFRA_API_KEY in your .env file or environment.\")\n",
    "\n",
    "DEEPINFRA_API_KEY = DEEPINFRA_API_KEY.strip()\n",
    "\n",
    "# === Config ===\n",
    "pdf_filename = \"sample.pdf\"\n",
    "pdf_url = \"https://www.princexml.com/samples/textbook/somatosensory.pdf\"\n",
    "# ✅ Fixed: No trailing space in URL\n",
    "deepinfra_api_url = \"https://api.deepinfra.com/v1/openai/embeddings\"\n",
    "model_name = \"Qwen/Qwen3-Embedding-8B\"\n",
    "batch_size = 8\n",
    "max_retries = 3\n",
    "delay_between_batches = 1  # Adjust based on rate limits\n",
    "\n",
    "# === Step 1: Download PDF if not present ===\n",
    "if not os.path.exists(pdf_filename):\n",
    "    print(f\"{pdf_filename} not found. Downloading...\")\n",
    "    response = requests.get(pdf_url)\n",
    "    response.raise_for_status()\n",
    "    with open(pdf_filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "else:\n",
    "    print(f\"{pdf_filename} found in current directory.\")\n",
    "\n",
    "with open(pdf_filename, \"rb\") as f:\n",
    "    pdf_file = BytesIO(f.read())\n",
    "\n",
    "# === Step 2: Extract text from PDF ===\n",
    "reader = PdfReader(pdf_file)\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    page_text = page.extract_text()\n",
    "    if page_text:\n",
    "        text += page_text + \"\\n\"\n",
    "\n",
    "# === Step 3: Preprocess text ===\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "text = text.strip()\n",
    "print(f\"Total extracted text length: {len(text)} characters\\n\")\n",
    "\n",
    "# === Step 4: DeepInfra Embedding Wrapper with Instruction Support ===\n",
    "class DeepInfraEmbeddings:\n",
    "    def __init__(self, api_key, model, batch_size=8, max_retries=3, delay=1):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.max_retries = max_retries\n",
    "        self.delay = delay\n",
    "        self.api_url = \"https://api.deepinfra.com/v1/openai/embeddings\"  # ✅ No trailing space\n",
    "\n",
    "    def _send_batch(self, texts):\n",
    "        # ✅ Add instruction as per Qwen3 recommendation\n",
    "        task = \"Given a textbook passage, retrieve related concepts\"\n",
    "        instructed_texts = [\n",
    "            f\"Instruct: {task}\\nQuery: {text}\" for text in texts\n",
    "        ]\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": \"Bearer {}\".format(self.api_key),\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        payload = {\n",
    "            \"input\": instructed_texts,\n",
    "            \"model\": self.model,\n",
    "            \"encoding_format\": \"float\"\n",
    "        }\n",
    "\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = requests.post(self.api_url, json=payload, headers=headers, timeout=30)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()[\"data\"]\n",
    "                    return [d[\"embedding\"] for d in data]\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Rate limited. Retrying in {2 ** attempt} seconds...\")\n",
    "                    time.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    print(f\"Error {response.status_code}: {response.text}\")\n",
    "                    time.sleep(2 ** attempt)\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Request failed (attempt {attempt + 1}): {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        raise RuntimeError(f\"Failed to get embeddings after {self.max_retries} retries.\")\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            print(f\"Embedding batch {i // self.batch_size + 1} / {len(texts) // self.batch_size + 1} (size: {len(batch)})\")\n",
    "            batch_embeddings = self._send_batch(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            if self.delay > 0 and i + self.batch_size < len(texts):\n",
    "                time.sleep(self.delay)\n",
    "        return all_embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# === Initialize Embedder ===\n",
    "embeddings = DeepInfraEmbeddings(\n",
    "    api_key=DEEPINFRA_API_KEY,\n",
    "    model=model_name,\n",
    "    batch_size=batch_size,\n",
    "    delay=delay_between_batches\n",
    ")\n",
    "\n",
    "print(f\"Using DeepInfra API with model: {model_name}\")\n",
    "\n",
    "# === Step 5: Initialize SemanticChunker ===\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except (ImportError, OSError):\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "semantic_text_splitter = SemanticChunker(\n",
    "    embeddings,\n",
    "    buffer_size=3,\n",
    "    breakpoint_threshold_type=\"percentile\",  # Try 90-95\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# === Step 6: Split into Paragraphs for Parallel Processing ===\n",
    "paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "paragraphs = [p.strip() for p in paragraphs if len(p.strip()) > 50]\n",
    "print(f\"Processing {len(paragraphs)} paragraphs...\\n\")\n",
    "\n",
    "# === Step 7: Parallel Semantic Chunking per Paragraph ===\n",
    "def chunk_paragraph(paragraph):\n",
    "    try:\n",
    "        docs = semantic_text_splitter.create_documents([paragraph])\n",
    "        return [\n",
    "            {\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata.copy()\n",
    "            }\n",
    "            for doc in docs\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing paragraph: {str(e)[:100]}...\")\n",
    "        return []\n",
    "\n",
    "semantic_chunks = []\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:  # Conservative to avoid rate limits\n",
    "    futures = [executor.submit(chunk_paragraph, p) for p in paragraphs]\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            semantic_chunks.extend(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Task failed: {e}\")\n",
    "\n",
    "# === Step 8: Final Results ===\n",
    "print(f\"\\n✅ Generated {len(semantic_chunks)} semantic chunks.\\n\")\n",
    "for i, chunk in enumerate(semantic_chunks[:3]):\n",
    "    preview = chunk[\"content\"][:300]\n",
    "    start_idx = chunk[\"metadata\"].get(\"start_index\", \"N/A\")\n",
    "    print(f\"Chunk {i+1} (Start Index: {start_idx})\")\n",
    "    print(f\"Content (preview): {preview}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513df75a",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a989c6",
   "metadata": {},
   "source": [
    "- The run using Tanenbaum's Computer Network book, is too slow for just semantic chunking, though the heaviest part seems to be at step 6, taking total of `22m 8.2s` with total of 1210 semantic chunks\n",
    "\n",
    "```text\n",
    "Generated 1210 semantic chunks.\n",
    "\n",
    "Chunk 1 (Start Index: 0)\n",
    "Content (preview): This page intentionally left blank COMPUTER NETWORKS FIFTH EDITION This page intentionally left blank COMPUTER NETWORKS FIFTH EDITION ANDREW S. TANENBAUM Vrije Universiteit Amsterdam, The Netherlands DAVID J. WETHERALL University of Washington Seattle, WA PRENTICE HALL Boston Columbus Indianapolis N...\n",
    "\n",
    "Chunk 2 (Start Index: 1187)\n",
    "Content (preview): Dulles Interior Illustrations : Laserwords, Inc. Media Editor : Daniel Sandin Composition : Andrew S....\n",
    "\n",
    "Chunk 3 (Start Index: 1288)\n",
    "Content (preview): Tanenbaum Copyeditor : Rachel Head Proofreader : Joe Ruddick Printer/Binder : Courier/Westford Cover Printer: Lehigh-Phoenix Color/ Hagerstown Credits and acknowledgments borrowed from other sources and reproduced, with permission, in this textbook appear on appropriate page within text. Many of the...\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The semantic chunking process, while effective, is computationally intensive and time-consuming for large documents. In the sample run, processing a moderately sized PDF resulted in over 1200 semantic chunks and required more than 22 minutes to complete. This highlights the need for further optimization or alternative approaches when scaling to full-length textbooks such as Tanenbaum's \"Computer Networks.\" Efficient batching, parallelization, or leveraging faster embedding models may be necessary for practical large-scale document analysis.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
